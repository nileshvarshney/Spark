{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "spark = SQLContext(SparkContext())\n",
    "#flight-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json('../data/flight-data/json/2015-summary.json').createOrReplaceTempView(\"flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flight\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   DEST_COUNTRY_NAME|sum(count)|\n",
      "+--------------------+----------+\n",
      "|             Senegal|        40|\n",
      "|              Sweden|       118|\n",
      "|               Spain|       420|\n",
      "|    Saint Barthelemy|        39|\n",
      "|Saint Kitts and N...|       139|\n",
      "|         South Korea|      1048|\n",
      "|        Sint Maarten|       325|\n",
      "|        Saudi Arabia|        83|\n",
      "|         Switzerland|       294|\n",
      "|         Saint Lucia|       123|\n",
      "|               Samoa|        25|\n",
      "|        South Africa|        36|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select DEST_COUNTRY_NAME, sum(count) from flight group by DEST_COUNTRY_NAME\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\")\\\n",
    ".where(\"`sum(count)` > 10\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Filter (isnotnull(sum(count)#115L) && (sum(count)#115L > 10))\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[sum(count#20L)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#18, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[partial_sum(count#20L)])\n",
      "         +- *(1) Project [DEST_COUNTRY_NAME#18, count#20L]\n",
      "            +- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#18) && StartsWith(DEST_COUNTRY_NAME#18, S))\n",
      "               +- *(1) FileScan json [DEST_COUNTRY_NAME#18,count#20L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/nilvarshney/github_nilesh/Spark/learn/data/flight-data/json/2015-su..., PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), StringStartsWith(DEST_COUNTRY_NAME,S)], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select DEST_COUNTRY_NAME, sum(count) from flight group by DEST_COUNTRY_NAME\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\")\\\n",
    ".where(\"`sum(count)` > 10\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   flight|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
